---
title: "Introduction to Stan"
author: "Monica Alexander"
date: "February 9 2021"
output: 
  pdf_document:
    number_sections: true
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

Today we will be starting off using Stan, looking at the kid's test score data set (available in resources for the [Gelman Hill textbook](https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html)). 

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
library(corrplot)
```


The data look like this:

```{r}
kidiq <- read_rds(here("data","kidiq.RDS"))
```
As well as the kid's test scores, we have a binary variable indicating whether or not the mother completed high school, the mother's IQ and age. 


# Descriptives

## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type

**Answer**:

* We want to explore the correlation between the kid_score and the mom_IQ first. First we could find the correlation between kid_score and mom_ID are the highest among the other three factors.
* Then then further explore the distribution of Mom_IQ and kids_score. Even though the correlation is high. But we find they have different type of distributions. The kid_score skew to the left and the mom's IQ skew to the right. Which is an interesting phenomenon.

```{r, echo=F}
corrplot(corr = cor(kidiq))
```


```{r, echo=F}
hist(kidiq$kid_score)
```


```{r, echo=F}
hist(kidiq$mom_iq)
```


# Estimating mean, no covariates

In class we were trying to estimate the mean and standard deviation of the kid's test scores. The `kids2.stan` file contains a Stan model to do this. If you look at it, you will notice the first `data` chunk lists some inputs that we have to define: the outcome variable `y`, number of observations `N`, and the mean and standard deviation of the prior on `mu`. Let's define all these values in a `data` list.


```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
```


Now we can run the model:

```{r, results= F}
fit <- stan(file = "/Users/siyiwei/Desktop/applied-stats-2021/code/models/kids2.stan", data = data)
```

Look at the summary

```{r}
fit
```

Traceplot

```{r}
traceplot(fit)
```

All looks fine. 

## Understanding output

What does the model actually give us? A number of samples from the posteriors. To see this, we can use `extract` to get the samples. 

```{r}
post_samples <- extract(fit)
```

This is a list, and in this case, each element of the list has 4000 samples. E.g. quickly plot a histogram of mu

```{r, echo=F}
hist(post_samples[["mu"]])
median(post_samples[["mu"]])
quantile(post_samples[["mu"]], 0.025)
quantile(post_samples[["mu"]], 0.975)
```



## Plot estimates

There are a bunch of packages, built-in functions that let you plot the estimates from the model, and I encourage you to explore these options (particularly in `bayesplot`, which we will most likely be using later on). I like using the `tidybayes` package, which allows us to easily get the posterior samples in a tidy format (e.g. using gather draws to get in long format). Once we have that, it's easy to just pipe and do ggplots as usual. `tidybayes` also has a bunch of fun visualizations, see more info here: https://mjskay.github.io/tidybayes/articles/tidybayes.html#introduction


Get the posterior samples for mu and sigma in long format:

```{r, echo=F}
dsamples <- fit %>%
  gather_draws(mu, sigma) 
dsamples
```

Let's plot the density of the posterior samples for mu and add in the prior distribution

```{r, echo=F}
dsamples %>% 
  filter(.variable == "mu") %>% 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
  
```

## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities.

* The posterior estimates are getting much closer to 80. Moreover, the standard deviation of the posterior decreases a huge amount. Meaning we have a much more concentrated posterior distribution. The plots on the prior and posterior densities could verify this result.

```{r, results = F}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 0.1

data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit <- stan(file = "/Users/siyiwei/Desktop/applied-stats-2021/code/models/kids2.stan", data = data)
```
```{r}
summary(fit)

dsamples <- fit %>%
  gather_draws(mu, sigma)

dsamples %>% 
  filter(.variable == "mu") %>% 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
```


# Adding covariates

Now let's see how kid's test scores are related to mother's education. We want to run the simple linear regression

$$
Score = \alpha + \beta X
$$
where $X = 1$ if the mother finished high school and zero otherwise. 

`kid3.stan` has the stan model to do this. Notice now we have some inputs related to the design matrix $X$ and the number of covariates (in this case, it's just 1).

Let's get the data we need and run the model. 



```{r, results = F}
X <- as.matrix(kidiq$mom_hs, ncol = 1)
K <- 1

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = "/Users/siyiwei/Desktop/applied-stats-2021/code/models/kids3.stan",
            data = data, 
            iter = 1000)
```

## Question 3

a) Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 

**Answer**: From the estimation of Stan model we could conclude alpha to be 78.02 and the estimation of beta to be 11.18. From the linear model we could conclude a similar result such that intercept to be 77.54 and slope to be 11.77.

b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?

**Answer**: They have a very strong linear correlation. Which could potentially reveal collinearity between alpha and beta. It is not good for our estimation for sure since they are not randomly distributed.

```{r, results=F}
fit2

lm_model <- lm(kid_score ~ mom_hs, data = kidiq)
print(lm_model$coefficients)

pars = c("alpha", "beta[1]")
pairs(fit2, pars = pars)
```


## Plotting results

It might be nice to plot the posterior samples of the estimates for the non-high-school and high-school mothered kids. Here's some code that does this: notice the `beta[condition]` syntax. Also notice I'm using `spread_draws`, because it's easier to calculate the estimated effects in wide format

```{r, echo=F}
fit2 %>%
  spread_draws(alpha, beta[condition], sigma) %>% 
     mutate(nhs = alpha, # no high school is just the intercept
          hs = alpha + beta) %>% 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") %>% 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeye() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")
  
```


## Question 4

Add in mother's IQ as a covariate and rerun the model. Please  mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 

**Answer**:

* After the center of the covariates. From Stan we could get the alpha to be 82.30, the mom_hs to be 5.72 and the mom iq to be 0.56
* From the coefficient. We could interpret as each unit of mum's IQ increase. It could improve the kid_score by 0.56 unit.

```{r, results=F}
X <- as.matrix(cbind(kidiq$mom_hs, kidiq$mom_iq - mean(kidiq$mom_iq)), ncol = 2)
K <- 2

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = "/Users/siyiwei/Desktop/applied-stats-2021/code/models/kids3.stan",
            data = data, 
            iter = 1000)
```


## Question 5 

Confirm the results from Stan agree with `lm()`

**Answer**:

* For the linear model. We could conclude the same result. The alpha is 86.79, the coefficient for mon_hs is 5.95 and for mom_iq is 0.563.

```{r, echo=F}
kidiq2 = kidiq
kidiq2$mom_hs = kidiq2$mom_hs - mean(kidiq2$mom_hs)
kidiq2$mom_iq = kidiq2$mom_iq - mean(kidiq2$mom_iq)
model2 <- lm(kid_score ~ mom_hs + mom_iq, data = kidiq2)
summary(model2)
```


## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 

**Answer**: The plot is shown below.

```{r}
library(purrr)
fit2 %>%
  spread_draws(alpha, beta[1], beta[2], sigma) %>% 
     mutate(nhs = alpha + as.numeric(map(beta,2))*110,
          hs = alpha + as.numeric(map(beta,1)) + as.numeric(map(beta,2))*110) %>% 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") %>% 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeye() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")
```



